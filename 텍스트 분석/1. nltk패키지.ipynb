{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf2d796",
   "metadata": {},
   "source": [
    "## NLTK패키지의 sent_tokenize를 이용하여 문장 토큰화를 진행\n",
    "### 1. 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9272a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taeeu\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\taeeu\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.el2c6ple4zyw3eceviv3oxxgrn2nrfm2.gfortran-win_amd64.dll\n",
      "C:\\Users\\taeeu\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taeeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed1195",
   "metadata": {},
   "source": [
    "## 2. 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2daf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 : ['The Matrix is everywhere its all around us,\\nhere even in this room.', 'you can see it out your window or on your television.', 'you feel it when you go to work, or go to church or pay your taxes.']\n",
      "문장 개수 : 3\n"
     ]
    }
   ],
   "source": [
    "text_sample = '''The Matrix is everywhere its all around us,\n",
    "here even in this room. you can see it out your window or on your television.\n",
    "you feel it when you go to work, or go to church or pay your taxes.'''\n",
    "\n",
    "# sent_tokenize(text=\"입력할 텍스트\")\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(\"결과 :\", sentences)\n",
    "\n",
    "# text가 몇개의 문장으로 되어있는지 개수를 세줌\n",
    "print(\"문장 개수 :\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967046f",
   "metadata": {},
   "source": [
    "## 3. 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c36685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "결과 : ['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(type(words))\n",
    "print(\"결과 :\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2f38b",
   "metadata": {},
   "source": [
    "## 4. 문장 토큰화와 단어 토큰화의 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34554b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['you', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['you', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    #문장별로 분리\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    #분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "#함수에 text_sample을 넣어줌\n",
    "word_tokens =  tokenize_text(text_sample)\n",
    "\n",
    "#반환값 word_tokens 출력\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86497272",
   "metadata": {},
   "source": [
    "## 5. StopWords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11437bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 'english'언어의 불용어 목록을 가져옴\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for i in word_tokens:\n",
    "    \n",
    "    filltered_words = []\n",
    "    \n",
    "    for word in i:\n",
    "        \n",
    "        #소문자로 변환\n",
    "        word=word.lower()\n",
    "        \n",
    "        #word가 불용어에 없다면 filltered_words에 추가\n",
    "        if word not in stopwords:\n",
    "            filltered_words.append(word)\n",
    "            \n",
    "    all_tokens.append(filltered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42990d3e",
   "metadata": {},
   "source": [
    "## 6. Stemming과 Lemmatization - 어근 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2390ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "#Stemming작업을 하는 LancasterStemmer을 stemmer라고 지칭하겠다는 명령!\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9779da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happiest happy\n",
      "fant fanciest\n",
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "#stemmer.stem('단어')를 통해서 stemming(어근추출)후 print\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('happiest'), stemmer.stem('happier'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))\n",
    "print(stemmer.stem('amuses'),stemmer.stem('amusing'), stemmer.stem('amused'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbdb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\taeeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Lemmatization작업을 하는 WordNetLemmatizer을 lemma라고 지칭하겠다는 명령!\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f776d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# lemma.lemmatize('단어', '품사')를 통해서 Lemmatization(어근 추출) 후 print\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
